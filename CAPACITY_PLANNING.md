# Capacity Planning for 1,000+ Users

## ‚úÖ Current Strengths

Your architecture can handle 1,000+ users because:
- **PostgreSQL** scales to millions of rows
- **Supabase** provides enterprise-grade infrastructure
- **Query-based matching** is efficient with proper indexes
- **Next.js** handles traffic well

---

## üìä Resource Estimates for 1,000 Users

### Database Storage
- **Profiles**: 1,000 users √ó ~2KB = ~2MB
- **Swipes**: Average 50 swipes/user = 50,000 rows √ó ~100 bytes = ~5MB
- **Matches**: Average 10 matches/user = 5,000 rows √ó ~100 bytes = ~500KB
- **Messages**: Average 100 messages/user = 100,000 rows √ó ~500 bytes = ~50MB
- **Notifications**: ~10,000 rows = ~1MB
- **Images**: 1,000 users √ó 2 photos √ó 2MB avg = **~4GB**

**Total Database**: ~60MB (without images)
**Total Storage**: ~4GB (with images in Supabase Storage)

### Supabase Free Tier Limits
- ‚úÖ **Database**: 500MB (you'll use ~60MB)
- ‚ö†Ô∏è **Storage**: 1GB (you'll need ~4GB - **UPGRADE NEEDED**)
- ‚úÖ **Bandwidth**: 5GB/month
- ‚ö†Ô∏è **Realtime connections**: 200 concurrent (you have 3 channels/user)

---

## üö® Action Items Before Launch

### 1. **Apply Performance Indexes** (CRITICAL)
```bash
# Run the performance_indexes.sql in Supabase SQL Editor
```
This will make queries **10-100x faster** with 1,000+ users.

### 2. **Upgrade Supabase Plan** (REQUIRED for 1,000 users)
- **Free Tier**: Supports ~250 users (1GB storage limit)
- **Pro Tier** ($25/mo): 
  - 8GB storage (enough for 2,000 users)
  - 50GB bandwidth
  - 500 concurrent realtime connections
  - Email support

### 3. **Optimize Real-time Subscriptions**
Currently each user has **3 active channels**:
- profiles-changes
- matches-updates  
- notifications-updates

**With 1,000 users online**: 3,000 connections (exceeds Pro tier limit)

**Solution**: Reduce to 1 combined channel:
```typescript
// Instead of 3 separate channels, use 1:
const channel = supabase
  .channel('user-updates')
  .on('postgres_changes', { event: '*', schema: 'public', table: 'profiles' }, handler)
  .on('postgres_changes', { event: '*', schema: 'public', table: 'matches' }, handler)
  .on('postgres_changes', { event: '*', schema: 'public', table: 'notifications' }, handler)
  .subscribe();
```

### 4. **Image Optimization**
Current setup: Photos stored in Supabase Storage

**Optimization**:
- Compress images to max 500KB each (use WebP format)
- This reduces 4GB to ~1GB for 1,000 users
- Add image compression on upload

### 5. **Query Optimization**
The matching query is already efficient, but verify with:
```sql
EXPLAIN ANALYZE
SELECT * FROM profiles 
WHERE status = 'approved' 
  AND gender = 'Female'
  AND preferred_gender IN ('Male', 'Everyone');
```

Should show "Index Scan" not "Seq Scan"

### 6. **Rate Limiting** (Already Implemented ‚úÖ)
You already have rate limiting on reports - good!

### 7. **Monitoring Setup**
Add monitoring for:
- Database query performance (slow queries)
- Storage usage
- Realtime connection count
- API request count

---

## üí∞ Cost Breakdown

### For 1,000 Active Users

**Supabase Pro** ($25/mo):
- 8GB database storage ‚úÖ
- 8GB file storage ‚úÖ (with image compression)
- 50GB bandwidth ‚úÖ
- 500 realtime connections ‚úÖ (if optimized to 1 channel/user)

**Total Monthly Cost**: **$25**

### For 5,000+ Users
You'd need:
- **Supabase Team** ($599/mo) or **Enterprise**
- Custom bandwidth
- More realtime connections

---

## üéØ Immediate Action Plan

**Before launch:**
1. ‚úÖ Run `performance_indexes.sql` (do this NOW)
2. ‚úÖ Test with 100 test accounts to verify performance
3. ‚úÖ Upgrade to Supabase Pro when you hit 200+ users
4. ‚úÖ Add image compression to reduce storage
5. ‚úÖ Optimize realtime channels to 1 per user

**After launch:**
1. Monitor Supabase dashboard for usage
2. Check slow query logs weekly
3. Optimize queries as needed
4. Consider CDN for images if bandwidth becomes issue

---

## ‚úÖ You're Ready!

Your architecture is **solid** for 1,000+ users. Just:
1. Apply the indexes
2. Upgrade to Pro when needed (~$25/mo)
3. Compress images
4. You're good to go! üöÄ
